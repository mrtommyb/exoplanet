.. module:: exoplanet

.. note:: This tutorial was generated from an IPython notebook that can be
          downloaded `here <../../_static/notebooks/gp.ipynb>`_.

.. _gp:



Scalable Gaussian processes in PyMC3
====================================

PyMC3 has support for `Gaussian Processes
(GPs) <https://docs.pymc.io/gp.html>`__, but this implementation is too
slow for many applications in time series astrophysics. So *exoplanet*
comes with an implementation of scalable GPs powered by
`celerite <https://celerite.readthedocs.io/>`__. More information about
the algorithm can be found in the `celerite
docs <https://celerite.readthedocs.io/>`__ and in the papers (`Paper
1 <https://arxiv.org/abs/1703.09710>`__ and `Paper
2 <https://arxiv.org/abs/1801.10156>`__), but this tutorial will give a
hands on demo of how to use celerite in PyMC3.

.. note:: For the best results, we generally recommend the use of the :class:`exoplanet.gp.terms.SHOTerm`, :class:`exoplanet.gp.terms.Matern32Term`, and :class:`exoplanet.gp.terms.RotationTerm` "terms" because the other terms tend to have unphysical behavior at high frequency.

Let’s start with the quickstart demo from the `celerite
docs <https://celerite.readthedocs.io/en/stable/tutorials/first/>`__.
We’ll fit the following simulated dataset using the sum of two
:class:`exoplanet.gp.terms.SHOTerm` objects.

First, generate the simulated data:

.. code:: python

    import numpy as np
    import matplotlib.pyplot as plt
    
    np.random.seed(42)
    
    t = np.sort(
        np.append(np.random.uniform(0, 3.8, 57), np.random.uniform(5.5, 10, 68))
    )  # The input coordinates must be sorted
    yerr = np.random.uniform(0.08, 0.22, len(t))
    y = (
        0.2 * (t - 5)
        + np.sin(3 * t + 0.1 * (t - 5) ** 2)
        + yerr * np.random.randn(len(t))
    )
    
    true_t = np.linspace(0, 10, 5000)
    true_y = 0.2 * (true_t - 5) + np.sin(3 * true_t + 0.1 * (true_t - 5) ** 2)
    
    plt.errorbar(t, y, yerr=yerr, fmt=".k", capsize=0, label="data")
    plt.plot(true_t, true_y, "k", lw=1.5, alpha=0.3, label="truth")
    plt.legend(fontsize=12)
    plt.xlabel("t")
    plt.ylabel("y")
    plt.xlim(0, 10)
    _ = plt.ylim(-2.5, 2.5)



.. image:: gp_files/gp_6_0.png


This plot shows the simulated data as black points with error bars and
the true function is shown as a gray line.

Now let’s build the PyMC3 model that we’ll use to fit the data. We can
see that there’s some roughly periodic signal in the data as well as a
longer term trend. To capture these two features, we will model this as
a mixture of two stochastically driven simple harmonic oscillators (SHO)
with the power spectrum:

.. math::


   S(\omega) = \sqrt{\frac{2}{\pi}}\frac{S_1\,{\omega_1}^4}{(\omega^2 - {\omega_1}^2)^2 + 2\,{\omega_1}^2\,\omega^2}
      + \sqrt{\frac{2}{\pi}}\frac{S_2\,{\omega_2}^4}{(\omega^2 - {\omega_2}^2)^2 + {\omega_2}^2\,\omega^2/Q^2}

The first term is :class:`exoplanet.gp.terms.SHOterm` with
:math:`Q=1/\sqrt{2}` and the second is regular
:class:`exoplanet.gp.terms.SHOterm`. This model has 5 free parameters
(:math:`S_1`, :math:`\omega_1`, :math:`S_2`, :math:`\omega_2`, and
:math:`Q`) and they must all be positive so we’ll fit for the log of
each parameter. Using *exoplanet*, this is how you would build this
model, choosing more or less arbitrary initial values for the
parameters.

.. code:: python

    import pymc3 as pm
    import theano.tensor as tt
    from exoplanet.gp import terms, GP
    
    with pm.Model() as model:
    
        mean = pm.Normal("mean", mu=0.0, sigma=1.0)
        logS1 = pm.Normal("logS1", mu=0.0, sigma=15.0, testval=np.log(np.var(y)))
        logw1 = pm.Normal("logw1", mu=0.0, sigma=15.0, testval=np.log(3.0))
        logS2 = pm.Normal("logS2", mu=0.0, sigma=15.0, testval=np.log(np.var(y)))
        logw2 = pm.Normal("logw2", mu=0.0, sigma=15.0, testval=np.log(3.0))
        logQ = pm.Normal("logQ", mu=0.0, sigma=15.0, testval=0)
    
        # Set up the kernel an GP
        kernel = terms.SHOTerm(log_S0=logS1, log_w0=logw1, Q=1.0 / np.sqrt(2))
        kernel += terms.SHOTerm(log_S0=logS2, log_w0=logw2, log_Q=logQ)
        gp = GP(kernel, t, yerr ** 2, mean=mean)
    
        # Condition the GP on the observations and add the marginal likelihood
        # to the model
        gp.marginal("gp", observed=y)

A few comments here:

1. The ``term`` interface in *exoplanet* only accepts keyword arguments
   with names given by the ``parameter_names`` property of the term. But
   it will also interpret keyword arguments with the name prefaced by
   ``log_`` to be the log of the parameter. For example, in this case,
   we used ``log_S0`` as the parameter for each term, but
   ``S0=tt.exp(log_S0)`` would have been equivalent. This is useful
   because many of the parameters are required to be positive so fitting
   the log of those parameters is often best.
2. The third argument to the :class:`exoplanet.gp.GP` constructor
   should be the *variance* to add along the diagonal, not the standard
   deviation as in the original `celerite
   implementation <https://celerite.readthedocs.io>`__.
3. Finally, the :class:`exoplanet.gp.GP` constructor takes an optional
   argument ``J`` which specifies the width of the problem if it is
   known at compile time. Just to be confusing, this is actually two
   times the ``J`` from `the celerite
   paper <https://arxiv.org/abs/1703.09710>`__. There are various
   technical reasons why this is difficult to work out in general and
   this code will always work if you don’t provide a value for ``J``,
   but you can get much better performance (especially for small ``J``)
   if you know what it will be for your problem. In general, most terms
   cost ``J=2`` with the exception of a
   :class:`exoplanet.gp.terms.RealTerm` (which costs `J=1`) and a
   :class:`exoplanet.gp.terms.RotationTerm` (which costs `J=4`).

To start, let’s fit for the maximum a posteriori (MAP) parameters and
look the the predictions that those make.

.. code:: python

    import exoplanet as xo
    
    with model:
        map_soln = xo.optimize(start=model.test_point)


.. parsed-literal::

    optimizing logp for variables: [logQ, logw2, logS2, logw1, logS1, mean]
    50it [00:00, 71.74it/s, logp=-2.569892e+00] 
    message: Optimization terminated successfully.
    logp: -41.9875698144546 -> -2.569892168573091


We’ll use the :func:`exoplanet.eval_in_model` function to evaluate the
MAP GP model.

.. code:: python

    with model:
        mu, var = xo.eval_in_model(
            gp.predict(true_t, return_var=True, predict_mean=True), map_soln
        )

.. code:: python

    plt.errorbar(t, y, yerr=yerr, fmt=".k", capsize=0, label="data")
    plt.plot(true_t, true_y, "k", lw=1.5, alpha=0.3, label="truth")
    
    # Plot the prediction and the 1-sigma uncertainty
    sd = np.sqrt(var)
    art = plt.fill_between(true_t, mu + sd, mu - sd, color="C1", alpha=0.3)
    art.set_edgecolor("none")
    plt.plot(true_t, mu, color="C1", label="prediction")
    
    plt.legend(fontsize=12)
    plt.xlabel("t")
    plt.ylabel("y")
    plt.xlim(0, 10)
    _ = plt.ylim(-2.5, 2.5)



.. image:: gp_files/gp_13_0.png


Now we can sample this model using PyMC3. There are strong covariances
between the parameters so we’ll use the custom
:func:`exoplanet.get_dense_nuts_step` to fit for these covariances
during burn-in.

.. code:: python

    with model:
        trace = pm.sample(
            tune=2000,
            draws=2000,
            start=map_soln,
            cores=2,
            chains=2,
            step=xo.get_dense_nuts_step(target_accept=0.9),
        )


.. parsed-literal::

    Multiprocess sampling (2 chains in 2 jobs)
    NUTS: [logQ, logw2, logS2, logw1, logS1, mean]
    Sampling 2 chains, 36 divergences: 100%|██████████| 8000/8000 [00:44<00:00, 178.95draws/s]
    There were 35 divergences after tuning. Increase `target_accept` or reparameterize.
    There was 1 divergence after tuning. Increase `target_accept` or reparameterize.
    The number of effective samples is smaller than 10% for some parameters.


Now we can compute the standard PyMC3 convergence statistics (using
``pymc3.summary``) and make a trace plot (using ``pymc3.traceplot``).

.. code:: python

    pm.traceplot(trace)
    pm.summary(trace)




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>mean</th>
          <th>sd</th>
          <th>hpd_3%</th>
          <th>hpd_97%</th>
          <th>mcse_mean</th>
          <th>mcse_sd</th>
          <th>ess_mean</th>
          <th>ess_sd</th>
          <th>ess_bulk</th>
          <th>ess_tail</th>
          <th>r_hat</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>mean</th>
          <td>0.006</td>
          <td>0.868</td>
          <td>-1.719</td>
          <td>1.568</td>
          <td>0.017</td>
          <td>0.017</td>
          <td>2555.0</td>
          <td>1306.0</td>
          <td>2578.0</td>
          <td>2294.0</td>
          <td>1.00</td>
        </tr>
        <tr>
          <th>logS1</th>
          <td>5.189</td>
          <td>3.471</td>
          <td>-0.544</td>
          <td>11.633</td>
          <td>0.102</td>
          <td>0.072</td>
          <td>1156.0</td>
          <td>1156.0</td>
          <td>629.0</td>
          <td>397.0</td>
          <td>1.01</td>
        </tr>
        <tr>
          <th>logw1</th>
          <td>-2.528</td>
          <td>1.324</td>
          <td>-5.135</td>
          <td>-0.076</td>
          <td>0.063</td>
          <td>0.045</td>
          <td>442.0</td>
          <td>442.0</td>
          <td>377.0</td>
          <td>119.0</td>
          <td>1.01</td>
        </tr>
        <tr>
          <th>logS2</th>
          <td>-4.176</td>
          <td>0.868</td>
          <td>-4.993</td>
          <td>-3.111</td>
          <td>0.101</td>
          <td>0.077</td>
          <td>74.0</td>
          <td>64.0</td>
          <td>422.0</td>
          <td>120.0</td>
          <td>1.01</td>
        </tr>
        <tr>
          <th>logw2</th>
          <td>1.129</td>
          <td>0.051</td>
          <td>1.032</td>
          <td>1.224</td>
          <td>0.001</td>
          <td>0.001</td>
          <td>2137.0</td>
          <td>2137.0</td>
          <td>2351.0</td>
          <td>1764.0</td>
          <td>1.00</td>
        </tr>
        <tr>
          <th>logQ</th>
          <td>2.951</td>
          <td>1.353</td>
          <td>1.102</td>
          <td>5.377</td>
          <td>0.101</td>
          <td>0.087</td>
          <td>180.0</td>
          <td>120.0</td>
          <td>472.0</td>
          <td>203.0</td>
          <td>1.01</td>
        </tr>
      </tbody>
    </table>
    </div>




.. image:: gp_files/gp_17_1.png


That all looks pretty good, but I like to make two other results plots:
(1) a `corner plot <https://corner.readthedocs.io>`__ and (2) a
posterior predictive plot.

The corner plot is easy using ``pymc3.trace_to_dataframe`` and I find it
useful for understanding the covariances between parameters when
debugging.

.. code:: python

    import corner
    
    samples = pm.trace_to_dataframe(trace)
    _ = corner.corner(samples)



.. image:: gp_files/gp_19_0.png


The “posterior predictive” plot that I like to make isn’t the same as a
“posterior predictive check” (which can be a good thing to do too).
Instead, I like to look at the predictions of the model in the space of
the data. We could have saved these predictions using a
``pymc3.Deterministic`` distribution, but that adds some overhead to
each evaluation of the model so instead, we can use
:func:`exoplanet.utils.get_samples_from_trace` to loop over a few
random samples from the chain and then the
:func:`exoplanet.eval_in_model` function to evaluate the prediction
just for those samples.

.. code:: python

    # Generate 50 realizations of the prediction sampling randomly from the chain
    N_pred = 50
    pred_mu = np.empty((N_pred, len(true_t)))
    pred_var = np.empty((N_pred, len(true_t)))
    with model:
        pred = gp.predict(true_t, return_var=True, predict_mean=True)
        for i, sample in enumerate(xo.get_samples_from_trace(trace, size=N_pred)):
            pred_mu[i], pred_var[i] = xo.eval_in_model(pred, sample)
    
    # Plot the predictions
    for i in range(len(pred_mu)):
        mu = pred_mu[i]
        sd = np.sqrt(pred_var[i])
        label = None if i else "prediction"
        art = plt.fill_between(true_t, mu + sd, mu - sd, color="C1", alpha=0.1)
        art.set_edgecolor("none")
        plt.plot(true_t, mu, color="C1", label=label, alpha=0.1)
    
    plt.errorbar(t, y, yerr=yerr, fmt=".k", capsize=0, label="data")
    plt.plot(true_t, true_y, "k", lw=1.5, alpha=0.3, label="truth")
    plt.legend(fontsize=12, loc=2)
    plt.xlabel("t")
    plt.ylabel("y")
    plt.xlim(0, 10)
    _ = plt.ylim(-2.5, 2.5)



.. image:: gp_files/gp_21_0.png


Citations
---------

As described in the :ref:`citation` tutorial, we can use
:func:`exoplanet.citations.get_citations_for_model` to construct an
acknowledgement and BibTeX listing that includes the relevant citations
for this model.

.. code:: python

    with model:
        txt, bib = xo.citations.get_citations_for_model()
    print(txt)


.. parsed-literal::

    This research made use of \textsf{exoplanet} \citep{exoplanet} and its
    dependencies \citep{exoplanet:exoplanet, exoplanet:foremanmackey17,
    exoplanet:foremanmackey18, exoplanet:pymc3, exoplanet:theano}.


.. code:: python

    print("\n".join(bib.splitlines()[:10]) + "\n...")


.. parsed-literal::

    
    @misc{exoplanet:exoplanet,
      author = {Daniel Foreman-Mackey and Ian Czekala and Rodrigo Luger and
                Eric Agol and Geert Barentsen and Tom Barclay},
       title = {exoplanet-dev/exoplanet v0.3.0},
       month = apr,
        year = 2020,
         doi = {10.5281/zenodo.1998447},
         url = {https://doi.org/10.5281/zenodo.1998447}
    }
    ...


